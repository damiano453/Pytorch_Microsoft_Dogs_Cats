{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "REBUILD_DATA = 0            #just in case of reimport\n",
    "IMG_SIZE = 50\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "device = (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ImportCatsDogs():\n",
    "    PATH = \"D:/Programming/NN_DATA/Dogs_Cats/PetImages/\"\n",
    "    CATS = \"Cat\"\n",
    "    DOGS = \"Dog\"\n",
    "    Labels = {CATS: 0, DOGS: 1}\n",
    "    \n",
    "    data_set = []\n",
    "    \n",
    "    dogs_count = 0\n",
    "    cats_count = 0\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def import_data_set(self,IMG_SIZE):\n",
    "        for label in self.Labels:\n",
    "            print(label)\n",
    "            for file in tqdm(os.listdir(self.PATH+label)):\n",
    "                if \".jpg\" in file:\n",
    "                    try:\n",
    "                        full_path = os.path.join(self.PATH,label,file)\n",
    "                        img = cv2.imread(full_path, cv2.IMREAD_GRAYSCALE)\n",
    "                        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "                        self.data_set.append([np.array(img), np.eye(2)[self.Labels[label]]])\n",
    "\n",
    "                        if self.Labels[label] == 0:\n",
    "                            self.cats_count += 1\n",
    "                        elif self.Labels[label] == 1:\n",
    "                            self.dogs_count += 1\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "        np.random.shuffle(self.data_set)\n",
    "        np.save(\"data_set.npy\", self.data_set)\n",
    "        print(\"Cats number: \", self.cats_count)\n",
    "        print(\"Dogs number: \", self.dogs_count)\n",
    "    \n",
    "\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self,pic_size=50):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,32,5)\n",
    "        self.conv2 = nn.Conv2d(32,64,5)\n",
    "        self.conv3 = nn.Conv2d(64,128,5)\n",
    "        self.pool = nn.MaxPool2d((2,2))\n",
    "        \n",
    "        self.__flat_size_ = None\n",
    "        x = torch.randn(50,50).view(-1,1,pic_size,pic_size)\n",
    "        \n",
    "        self.convs(x)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.__flat_size_, 512)\n",
    "        self.fc2 = nn.Linear(512, 16)\n",
    "        self.fc3 = nn.Linear(16, 2)\n",
    "    \n",
    "    def convs(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)                #this what we put to fc nn\n",
    "     \n",
    "        if self.__flat_size_ is None:\n",
    "            self.__flat_size_ = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "            print(\"Flat Size: \",self.__flat_size_)\n",
    "        return x\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1,self.__flat_size_)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.log_softmax(self.fc3(x),1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "if REBUILD_DATA == 1:\n",
    "    importcatsdogs = ImportCatsDogs()\n",
    "    importcatsdogs.import_data_set(IMG_SIZE)\n",
    "\n",
    "data_set = np.load(\"data_set.npy\", allow_pickle=True)\n",
    "    \n",
    "\n",
    "inputs = torch.Tensor([data_set[:,0]]).view(-1,IMG_SIZE,IMG_SIZE)\n",
    "labels = torch.Tensor([data_set[:,1]])\n",
    "\n",
    "\n",
    "########## poki co, zmien na shuffle\n",
    "len_of_data_set = labels.size()[1]\n",
    "len_of_learnig = 0.1\n",
    "\n",
    "train_X = inputs[int(len_of_data_set*len_of_learnig):]\n",
    "train_Y = labels[0][int(len_of_data_set*len_of_learnig):]\n",
    "\n",
    "test_X = inputs[:int(len_of_data_set*len_of_learnig)]\n",
    "test_Y = labels[0][:int(len_of_data_set*len_of_learnig)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flat Size:  512\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n",
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 113/113 [00:22<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 113/113 [00:21<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 113/113 [00:21<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 113/113 [00:22<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 113/113 [00:21<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 113/113 [00:21<00:00,  5.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 113/113 [00:21<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 113/113 [00:21<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 113/113 [00:23<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 113/113 [00:21<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 113/113 [00:21<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 113/113 [00:21<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 113/113 [00:21<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12. Loss: 1.6735540628433228. ACC: 0.5384615384615384\n"
     ]
    }
   ],
   "source": [
    "net           = Net().to(device)\n",
    "optimizer     = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss().to(device)\n",
    "\n",
    "print(net)\n",
    "\n",
    "def forward_learn_acc(X,Y,loss_function,optimizer,learn = False):\n",
    "    if learn == True:\n",
    "        net.zero_grad()\n",
    "  \n",
    "    outputs = net(X)\n",
    "    matches = [i.argmax() == j.argmax() for i,j in zip(outputs,Y)]\n",
    "\n",
    "    accuraccy = matches.count(True)/len(matches)\n",
    "    \n",
    "    if learn == True:\n",
    "        loss = loss_function(outputs, Y)  \n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            loss = loss_function(outputs, Y)  \n",
    "    \n",
    "    return loss, accuraccy\n",
    "\n",
    "\n",
    "def train(net,optimizer,loss_function):\n",
    "    BATCH_SIZE = 200\n",
    "    EPOCHS     = 13\n",
    "    \n",
    "    MODEL_NAME = f\"IMG_SIZE_{IMG_SIZE}__EPOCHS_{EPOCHS}__t_{round(time.time(),3)}\"\n",
    "    \n",
    "    loss = 0\n",
    "    accurracy = 0\n",
    "\n",
    "    with open(f\"model_{MODEL_NAME}.log\", \"a\") as file:\n",
    "        file.write(f\"Epoch,Iteration,Train Loss,Train Accuracy,Validation Loss,Validation Accuracy\\n\")\n",
    "        for epoch in range(EPOCHS):\n",
    "            print(\"Epoch: \",epoch)\n",
    "            for batch_iter in tqdm(range(0,len(train_X),BATCH_SIZE)):\n",
    "                batch_X = train_X[batch_iter:batch_iter+BATCH_SIZE].view(-1,1,IMG_SIZE,IMG_SIZE).to(device)\n",
    "                batch_Y = train_Y[batch_iter:batch_iter+BATCH_SIZE].to(device)\n",
    "\n",
    "                loss, accuracy = forward_learn_acc(batch_X,batch_Y,loss_function,optimizer, learn = True)#,learn = True)\n",
    "                if batch_iter % 2000 == 0:    \n",
    "                    indexes = torch.randperm(BATCH_SIZE)\n",
    "                    batch_valid_X = test_X[indexes].view(-1,1,IMG_SIZE,IMG_SIZE).to(device)\n",
    "                    batch_valid_Y = test_Y[indexes].to(device)\n",
    "\n",
    "                    val_loss, val_accuracy = forward_learn_acc(batch_valid_X, batch_valid_Y,loss_function,optimizer, learn = False)\n",
    "\n",
    "                    file.write(f\"{round(int(epoch),6)},{round(int(batch_iter),6)},{round(float(loss),6)},{round(float(accuracy),6)},{round(float(val_loss),6)},{round(float(val_accuracy),6)}\\n\")\n",
    "                    pass\n",
    "\n",
    "        print(f\"Epoch: {epoch}. Loss: {loss}. ACC: {accuracy}\")\n",
    "    torch.save(net.state_dict(), f\"net__{MODEL_NAME}.pth\")\n",
    "    #torch.cuda.empty_cache()        \n",
    "    #del net, optimizer, loss_function, outputs, batch_X, batch_Y, loss   \n",
    "\n",
    "train(net,optimizer,loss_function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NN defined\n",
      "Flat Size:  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 54.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6715) 0.7321571772253408\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "\n",
    "matches     = 0\n",
    "loss        = 0\n",
    "\n",
    "try:\n",
    "    print(net3)\n",
    "except:\n",
    "    print(\"No NN defined\")\n",
    "    net = Net()\n",
    "    net.load_state_dict(torch.load(\"net__MODEL_NAME.pth\"))\n",
    "    net.eval() \n",
    "    net.to(device)\n",
    "\n",
    "\n",
    "\n",
    "for batch_iter in tqdm(range(0,len(test_X),BATCH_SIZE)):\n",
    "    batch_test_X = test_X[batch_iter:batch_iter+BATCH_SIZE].view(-1,1,IMG_SIZE,IMG_SIZE).to(device)\n",
    "    batch_test_Y = test_Y[batch_iter:batch_iter+BATCH_SIZE].to(device)\n",
    "    with torch.no_grad():\n",
    "        if batch_iter == 0:\n",
    "            outputs = net(batch_test_X)\n",
    "        else:\n",
    "            outputs = torch.cat((outputs, net(batch_test_X)))\n",
    "with torch.no_grad():\n",
    "    matches = [i.argmax() == j.argmax() for i,j in zip(outputs,test_Y)]\n",
    "    accuraccy = matches.count(True)/len(matches)\n",
    "    loss = loss_function(outputs.cpu(), test_Y)\n",
    "\n",
    "print(loss,accuraccy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
